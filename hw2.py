# -*- coding: utf-8 -*-
"""hw2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KhtiCF_mHB0wgy1_fYwmWGANEdMvebwr

# Setup
"""

import math
import torch
import sys
import os
import numpy as np


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device

model_path = '.' # where the model.py files live
sys.path.append(os.path.abspath(model_path))

from model import Transformer
from model import ModelArgs

import fairscale.nn.model_parallel.initialize as fs_init
from fairscale.nn.model_parallel.layers import (
    ParallelEmbedding,
    RowParallelLinear,
    ColumnParallelLinear,
)

def setup_model_parallel():
    local_rank = 0
    world_size = 1

    os.environ["RANK"] = "0"
    os.environ["WORLD_SIZE"] = "1"
    os.environ["LOCAL_RANK"] = "0"
    os.environ["MASTER_ADDR"] = "localhost"
    os.environ["MASTER_PORT"] = "29500"
    torch.distributed.init_process_group(backend="nccl", world_size=1, rank=0)
    fs_init.initialize_model_parallel(world_size)
    torch.cuda.set_device(local_rank)

    # seed must be the same in all processes
    torch.manual_seed(1)
    return local_rank, world_size

rank, size = setup_model_parallel()

import zstandard as zstd
from transformers import AutoTokenizer

"""# Hyperparameters"""

test_path = os.path.join(model_path, 'test.jsonl.zst')
val_path = os.path.join(model_path, 'val.jsonl.zst')
train_path = os.path.join(model_path, '00.jsonl.zst')
print(train_path)

batch_size = 12

# train loop config
eval_interval = 10
log_interval = 1

# StepLR parameters
step_size = 30
gamma = 0.1

# val loss config
eval_iters = 20

# optimizer config (AdamW)
lr=6e-4
weight_decay=1e-1
beta1 = 0.9
beta2 = 0.95

"""# Load and Tokenize Data"""

import json
import io

def read_line(line):
  line = line.strip()
  try:
    data = json.loads(line)
  except ValueError:
    return None
  return data['text']

def read_file(file_path, gbs=1.0, entries=10000):
  with open(file_path, 'rb') as file:
    decompressor = zstd.ZstdDecompressor()
    stream_reader = decompressor.stream_reader(file)
    stream = io.TextIOWrapper(stream_reader, encoding='utf-8')

    lines = []
    for line in stream:
      line = read_line(line)
      if line is not None:
        lines.append(line)
      if len(lines) == entries:
        break
  return lines

# https://huggingface.co/transformers/v3.0.2/preprocessing.html#base-use
tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')
encoding = tokenizer("Hello I am a human")
print(tokenizer.vocab_size)
print(encoding)

eval_dataset = tokenizer(read_file(val_path, entries=10000), padding=True, truncation=True, return_tensors="pt")['input_ids']
train_dataset = tokenizer(read_file(train_path, entries=30000), padding=True, truncation=True, return_tensors="pt")['input_ids']
print(type(eval_dataset))
eval_dataset.to(device)
train_dataset.to(device)

def get_batch(data: np.ndarray):
    ix = torch.randint(len(data) - 1, (batch_size,))
    x = torch.stack([torch.from_numpy((data[i]).numpy()[:-1]) for i in ix]) # first n - 1
    y = torch.stack([torch.from_numpy((data[i]).numpy()[1:]) for i in ix]) # last n - 1
    return x, y

print(get_batch(train_dataset))

"""# Training"""

from torch.optim.lr_scheduler import StepLR
from torch.nn.utils import clip_grad_norm_
from torch.autograd import Variable
import gc

def train(
    model: torch.nn.Module,
    optimizer: torch.optim.Optimizer,
    train_data: np.ndarray,
    val_data: np.ndarray,
    grad_clip: float = 1.0,
    checkpoint = None,
    iters: int = 2,
    out_dir: str = 'version1'
) -> None:

    train_losses = []
    val_losses = []
    out_dir = os.path.join(model_path, out_dir)
    start_iter = 0

    scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)

    if checkpoint:
      train_losses = checkpoint['train_loss']
      val_losses = checkpoint['val_loss']
      start_iter = checkpoint['start_iter']

      model.load_state_dict(checkpoint['model'])
      scheduler.load_state_dict(checkpoint['scheduler'])

    
    for i in range(start_iter, start_iter + iters):
        scheduler.step()  # Update learning rate

        # evaluate the loss on train/val sets and write checkpoints
        if i > 0 and i % eval_interval == 0:
            val_loss = validate(model, val_data)
            dt = time.time() - t0
            if i % log_interval == 0:
                print(f"Validating iter {i}: loss {val_loss:.4f}, time: {dt*1000:.2f}ms")
            val_losses.append(val_loss)

        t0 = time.time()

        input_ids, targets = get_batch(train_dataset)  # type: ignore[union-attr,arg-type]
        input_ids = input_ids.to(device)
        targets = targets.to(device)
        logits = model.forward(input_ids, 0)
        logits = torch.flatten(logits, start_dim = 0, end_dim = 1)

        loss = torch.nn.functional.cross_entropy(logits, targets.reshape(-1), ignore_index=tokenizer._pad_token_type_id)
        loss.backward()
        loss = loss.detach().cpu().numpy()
        train_losses.append(loss)

        # Save checkpoint after getting training loss of this iteration
        if i > 0 and i % eval_interval == 0:
          print(f"Saving checkpoint to {out_dir}")
          torch.save({
              'train_loss': train_losses,
              'val_loss': val_losses,
              'start_iter': i + 1,
              'model': model.state_dict(),
              'scheduler': scheduler.state_dict()
          }, os.path.join(out_dir, f"{i:04}-ckpt.pt")) 

        # Gradient clipping
        if grad_clip != 0.0:
            clip_grad_norm_(model.parameters(), grad_clip)

        optimizer.step()
        optimizer.zero_grad()

        # Logging
        dt = time.time() - t0
        if i % log_interval == 0:
            print(f"Training iter {i}: loss {loss.item():.4f}, time: {dt*1000:.2f}ms")
        gc.collect()
        torch.cuda.empty_cache()

@torch.no_grad()
def validate(model: torch.nn.Module, val_data: np.ndarray) -> torch.Tensor:
    print("Validating ...")
    model.eval()  # change to eval mode
    losses = torch.zeros(eval_iters)
    for k in range(eval_iters):
        input_ids, targets = get_batch(eval_dataset)  # type: ignore[union-attr,arg-type]
        input_ids = input_ids.to(device)
        targets = targets.to(device)
        logits = model.forward(input_ids, 0)
        logits = torch.flatten(logits, start_dim=0, end_dim=1)
        loss = torch.nn.functional.cross_entropy(logits, targets.reshape(-1))
        loss = loss.detach().cpu().numpy()
        losses[k] = loss.item()
        gc.collect()
        torch.cuda.empty_cache()
    out = losses.mean()
    model.train() # change back to train mode
    return out

# set up checkpoint to continue from
checkpoint = None
# file = f"/content/drive/Shareddrives/493S_hw2/ckpts/" + sorted(os.listdir(f"/content/drive/Shareddrives/493S_hw2/ckpts"))[-1]
# checkpoint = torch.load(file)

# determine model version and output directory of checkpoints
out_dir = 'ckpts'
# !mkdir {model_path}/{out_dir}
# !ls {model_path}

import time
def main():
    torch.cuda.empty_cache()
    model = Transformer(ModelArgs())
    model.to(device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay, betas=(beta1, beta2))
    
    train(model, optimizer, train_dataset, eval_dataset, checkpoint=None, iters=11, out_dir=out_dir)

main()

"""# Results"""

import matplotlib.pyplot as plt
# get latest checkpoint
file = os.path.join(model_path, out_dir, sorted(os.listdir(os.path.join(model_path, out_dir)))[-1])
checkpoint = torch.load(file)
print(file)
train_losses = checkpoint['train_loss']
val_losses = checkpoint['val_loss']

train_losses = [loss.item() for loss in train_losses]
print(len(train_losses))
val_losses = [loss.item() for loss in val_losses]

plt.figure(figsize=(10,5))
plt.title("Loss during training")
plt.plot(train_losses,label="Training loss")
plt.plot((np.arange(len(val_losses)) + 1) * 10, val_losses, label="Validation loss")
plt.xlabel("iterations")
plt.ylabel("Loss")
# plt.ylim((0, 20))
plt.legend()
plt.savefig(os.path.join(model_path, "plots"))

"""# Testing"""

# sys.path.append(os.path.abspath(model_path))
from generation import LLaMA

file = os.path.join(model_path, out_dir, sorted(os.listdir(os.path.join(model_path, out_dir)))[-1])
checkpoint = torch.load(file)
model = Transformer(ModelArgs())
model.load_state_dict(checkpoint, strict=False)

generator = LLaMA(model, tokenizer)

print(dir(tokenizer))

prompts = [
        # For these prompts, the expected answer is the natural continuation of the prompt
        "I believe the meaning of life is",
        "Simply put, the theory of relativity states that ",
        "Building a website can be done in 10 simple steps:\n",
        # Few shot prompts: https://huggingface.co/blog/few-shot-learning-gpt-neo-and-inference-api
        """Tweet: "I hate it when my phone battery dies."
          Sentiment: Negative
          ###
          Tweet: "My day has been ðŸ‘"
          Sentiment: Positive
          ###
          Tweet: "This is the link to the article"
          Sentiment: Neutral
          ###
          Tweet: "This new music video was incredibile"
          Sentiment:""",
                  """Translate English to French:

          sea otter => loutre de mer

          peppermint => menthe poivrÃ©e

          plush girafe => girafe peluche

          cheese =>""",
        ]
results = generator.generate(
    [tokenizer.encode(x) for x in prompts], max_gen_len=256
)

for result in results:
    print(result)
    print("\n==================================\n")


"""# Unused stuff"""

# def get_lr(it, warmup_iters, lr_decay_iters, learning_rate, min_lr):
#     # 1) linear warmup for warmup_iters steps
#     if it < warmup_iters:
#         return learning_rate * it / warmup_iters
#     # 2) if it > lr_decay_iters, return min learning rate
#     if it > lr_decay_iters:
#         return min_lr
#     # 3) in between, use cosine decay down to min learning rate
#     decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)
#     assert 0 <= decay_ratio <= 1
#     coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))  # coeff ranges 0..1
#     return min_lr + coeff * (learning_rate - min_lr)

# import numpy as np

# def estimate_loss(model, dataset, eval_dataset_loader=None):
#     out = {}
#     model.eval()
#     device = torch.cuda()
    
#     eval_losses = []
#     for inputs_ in eval_dataset_loader:
#         X = inputs_['input_ids'].to(device)
#         Y = inputs_['labels'].to(device)
#         with torch.no_grad():
#             logits, loss = model(X, Y)
        
#         this_batch_size = X.shape[0]
#         eval_losses.extend([loss.item()] * this_batch_size)
    
#     out['val'] = np.mean(eval_losses)
#     out['train'] = 0.0

#     model.train()
#     return out

# !pip install wandb
# import time
# import os
# import torch
# import wandb
# import numpy as np

# def train_transformer_model(
#     dataset,
#     train_data,
#     val_data,
#     get_batch,
#     model,
#     optimizer,
#     iter_num,
#     best_val_loss,
#     batch_size,
#     gradient_accumulation_steps,
#     decay_lr,
#     learning_rate,
#     eval_interval,
#     wandb_log,
#     out_dir,
#     log_interval,
#     max_iters,
#     wandb_project
# ):
#     # training loop
#     t0 = time.time()
#     local_iter_num = 0  # number of iterations in the lifetime of this process
#     raw_model = model  # unwrap DDP container if needed
#     running_mfu = -1.0

#     if batch_size > 1:
#         gradient_accumulation_steps = 1  # in this case, we want to update fast.

#     print("Starting training loop...")

#     wandb_run_name = 'ft-' + '-' + str(batch_size) + '-' + str(learning_rate)

#     if wandb_log:
#         wandb.init(project=wandb_project, name=wandb_run_name)

#     X, Y = get_batch('train')  # fetch the very first batch

#     while True:
#         # determine and set the learning rate for this iteration
#         lr = get_lr(iter_num, warmup_iters=400, lr_decay_iters=5000, learning_rate=6e-4, min_lr=6e-5)
#         print(f"step {iter_num}: learning rate {lr:.2e}")
#         for param_group in optimizer.param_groups:
#             param_group['lr'] = lr

#         # evaluate the loss on train/val sets and write checkpoints
#         if iter_num % eval_interval == 0:
#             losses = estimate_loss(model, dataset, val_data)
#             print(f"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}")
#             if wandb_log:
#                 wandb.log({
#                     "iter": iter_num,
#                     "train/loss": losses['train'],
#                     "val/loss": losses['val'],
#                     "lr": lr,
#                     "mfu": running_mfu * 100,  # convert to percentage
#                 })
#             if losses['val'] < best_val_loss:
#                 best_val_loss = losses['val']
#                 if iter_num > 0:
#                     checkpoint = {
#                         'model': raw_model.state_dict(),
#                         'optimizer': optimizer.state_dict(),
#                         'iter_num': iter_num,
#                         'best_val_loss': best_val_loss,
#                         'config': {
#                             'dataset': dataset,
#                             'out_dir': out_dir,
#                             'wandb_log': wandb_log,
#                         },
#                     }
#                     print(f"saving checkpoint to {out_dir}")
#                     torch.save(checkpoint, os.path.join(out_dir, f'ckpt_{wandb_run_name}.pt'))

#         # forward backward update, with optional gradient accumulation to simulate larger batch size
#         # and using the GradScaler if data type is float16
#         for micro_step in range(gradient_accumulation_steps):
#             logits = model(X, Y)
#             shift_logits = logits[..., :-1, :].contiguous()
#             # What is targets?
#             shift_targets = Y[..., 1:].contiguous()
#             loss = F.cross_entropy(shift_logits.view(-1, shift_logits.size(-1)), shift_targets.view(-1), ignore_index=-100)

#             # immediately async prefetch next batch while model is doing the forward pass on the GPU
#             X, Y = get_batch('train')

#             # backward pass, with gradient scaling if training in fp16
#             loss.backward()

#         # step the optimizer and scaler if training in fp16
#         optimizer.step()

#         # flush the gradients as soon as we can, no need for this memory anymore
#         optimizer.zero_grad(set_to_none=True)

#         # timing and logging
#         t1 = time.time()
#         dt = t1 - t0
#         t0 = t1

#         if iter_num % log_interval == 0:
#             lossf = loss.item()  # loss as float. note: this is a CPU-GPU sync point
#             if local_iter_num >= 5:  # let the training loop settle a bit
#                 mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)
#                 running_mfu = mfu if running_mfu == -1.0 else 0.9 * running_mfu + 0.1 * mfu
#             print(f"iter {iter_num}: loss {lossf:.4f}, time {dt * 1000:.2f}ms, mfu {running_mfu * 100:.2f}%")
#         iter_num += 1
#         local_iter_num += 1

#         # termination conditions
#         if iter_num > max_iters:
#             break

# from torch.nn import functional as F
# logits = Transformer.forward(get_batch(data[0], 10, 32), 0)
# targets = Transformer.forward(get_batch(data, 10, 32), 1)
# shift_logits = logits[..., :-1, :].contiguous()
# shift_targets = targets[..., 1:].contiguous()
# loss = F.cross_entropy(shift_logits.view(-1, shift_logits.size(-1)), shift_targets.view(-1), ignore_index=-100)